{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\DANG XUAN\n",
      "[nltk_data]     BACH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\DANG XUAN\n",
      "[nltk_data]     BACH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Natural Language Toolkit (nltk)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import download\n",
    "download('stopwords')\n",
    "download('wordnet')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining dictionary containing all emojis with their meanings.\n",
    "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
    "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
    "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordlist = stopwords.words('english') + mystopwordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(textdata):\n",
    "    processedText = []\n",
    "    #creating a Lemmatizer\n",
    "    wordLemma = WordNetLemmatizer() #define the imported library\n",
    "    # Defining regular expression pattern we can find. in tweets\n",
    "    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\" \n",
    "    userPattern       = '@[^\\s]+' # e.g @FagbamigbeK check this out\n",
    "    alphaPattern      = \"[^a-zA-Z0-9]\" # e.g I am *10 better!\n",
    "    sequencePattern   = r\"(.)\\1\\1+\"  # e.g Heyyyyyyy, I am back!\n",
    "    seqReplacePattern = r\"\\1\\1\" # e.g Replace Heyyyyyyy with Heyy\n",
    "    \n",
    "    for tweet in textdata:\n",
    "        tweet = tweet.lower() #normalizing all text to a lower case\n",
    "        # Replace all URls with 'URL'\n",
    "        tweet = re.sub(urlPattern,' URL',tweet) \n",
    "        # Replace all emojis.\n",
    "        for emoji in emojis.keys():\n",
    "            tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])  \n",
    "        # Replace @USERNAME to 'USER'.\n",
    "        tweet = re.sub(userPattern,' USER', tweet)  \n",
    "        # Replace all non alphabets.\n",
    "        tweet = re.sub(alphaPattern, \" \", tweet) # e.g I am *10 better!\n",
    "        # Replace 3 or more consecutive letters by 2 letter.\n",
    "        tweet = re.sub(sequencePattern, seqReplacePattern, tweet) # e.g Replace Heyyyyyyy with Heyy\n",
    "         \n",
    "        tweetwords = ''\n",
    "        for word in tweet.split():\n",
    "            if len(word) > 2 and word.isalpha():\n",
    "                word = wordLemma.lemmatize(word)\n",
    "                tweetwords += (word + ' ')\n",
    "        processedText.append(tweetwords)\n",
    "    return processedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_models():\n",
    "#     # Load the vectoriser.\n",
    "#     file = open('./vectoriser-ngram-(1,2).pickle', 'rb')\n",
    "#     vectoriser = pickle.load(file)\n",
    "#     file.close()\n",
    "    \n",
    "#     # Load the LGBM Model.\n",
    "#     file = open('./Sentiment-LightGBM.pickle', 'rb')\n",
    "#     LGBMmodel = pickle.load(file)\n",
    "#     file.close()\n",
    "    \n",
    "#     return vectoriser, LGBMmodel\n",
    "\n",
    "def predict(vectoriser, model, text):\n",
    "    # Predict the sentiment\n",
    "    textdata = vectoriser.transform(preprocess(text)) \n",
    "    sentiment = model.predict(textdata)\n",
    "    \n",
    "    # Make a list of text with sentiment.\n",
    "    data = []\n",
    "    for text, pred in zip(text, sentiment):\n",
    "        data.append((text,pred))\n",
    "        \n",
    "    # Convert the list into a Pandas DataFrame.\n",
    "    df = pd.DataFrame(data, columns = ['text','sentiment'])\n",
    "    df = df.replace([0,1], [\"Negative\",\"Positive\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DANG XUAN BACH\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.3.2 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\DANG XUAN BACH\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.3.2 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Để load lại:\n",
    "import joblib\n",
    "vectoriser = joblib.load('tfidf_vectoriser.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DANG XUAN BACH\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.3.2 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Mở tệp chứa mô hình đã lưu\n",
    "with open('Sentiment-LightGBM.pickle', 'rb') as file:\n",
    "    LGBMmodel = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text sentiment\n",
      "0                                 Today is so great!  Positive\n",
      "1                     May the Good Lord be with you.  Positive\n",
      "2                                    I hate peanuts!  Negative\n",
      "3  Mr. Kehinde, what are you doing next? this is ...  Positive\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\": \n",
    "    # Text to classify should be in a list.\n",
    "    text = [\"Today is so great!\",\n",
    "            \"May the Good Lord be with you.\", \"I hate peanuts!\",\n",
    "            \"Mr. Kehinde, what are you doing next? this is great!\"]\n",
    "    \n",
    "    df = predict(vectoriser, LGBMmodel, text)\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Đọc dữ liệu từ file CSV vào DataFrame\n",
    "# df = pd.read_csv(\"C:\\\\Users\\\\Admin\\\\Documents\\\\TLCN\\\\data\\\\hashtag_joebiden.csv\")\n",
    "\n",
    "# # Xác định cột chứa các câu bạn muốn kiểm tra\n",
    "# text_column = 'tweet'\n",
    "# df[text_column] = df[text_column].fillna('')\n",
    "\n",
    "# # Lấy dữ liệu từ cột chứa các câu\n",
    "# text_to_predict = df[text_column].tolist()\n",
    "\n",
    "# # Thực hiện dự đoán bằng cách sử dụng hàm predict đã được định nghĩa trước đó\n",
    "# result_df = predict(vectoriser, LGBMmodel, text_to_predict)\n",
    "\n",
    "# # In kết quả\n",
    "# print(result_df.head())\n",
    "\n",
    "# # Lưu kết quả vào một tệp CSV nếu cần\n",
    "# result_df.to_csv('predicted_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"created_at\":\"2024-03-14 20:30:19\", \"tweet_id\": \"1768268422909939980\", \"tweet\": \"Guardrails vs. Microsoft Guidance: A Hands-On Tutorial for LLM Output Validation by Daniel Klitzke at #ITNEXT. #python #llm #mlops #machinelearning #datascience https://t.co/z7htmzTEQF\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit --master local[*] --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.4.0,mysql:mysql-connector-java:8.0.33 ab.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
